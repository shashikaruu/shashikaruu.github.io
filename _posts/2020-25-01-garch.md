---
title: "Improving Volatility Forecasting with Machine Learning"
date: 2018-01-28
tags: [machine learning, history, time series]
header:
excerpt: "Machine Learning, History, Time Series"
---

Dynamic volatility plays an important role in modern financial time series. the application of forecasting conditional variance is essential for investors to make decisions such as risk assessment, leverage effects evaluation and the pricing of derivative securities.

The following outlines how we can incorporate straight-forward machine learning techniques into existing time-series econometrics, to significantly improve forecasting accuracy.

## Forecasting Volatility

I employ the conditional variances of daily logarithmic rates of return of the Australia All Ordinates (Live) Index. Here, I compute the daily realized variances from the 5-minute log-returns of the Index.

```r
library(fImport)
library(timeDate)
library(FinTS)
library(ccgarch)
library(vars)
library(rugarch)

load("AORD.RData")
plot.ts(AORD)

```


![svg]({{ site.url }}/images/data.png)

To tests for forecasting performance, I'll perform a rolling sample forecasting exercise of a 5-day-ahead conditional volatility forecasts. The main criteria I'll use is the Root Mean Square Prediction Error (RMPSE).

## The Standard MODEL

The autocorrelation functions for log returns indicated several potential autocorrelations including at lag 5 and lag 9. Information Criterion (IC) reveals that there is potential for log returns autocorrelation up to 5 lags.

For realised volatility, the data exhibits strong persistence in its autocorrelation function. IC reveals that is potential for a up to 28 lags in realised volatility, which is likely due to the ARCH effects.

Initial tests suggest modelling the data through an AR-GARCH framework, starting with an AR(1), GARCH(1,1) model. We then explore variations around this model, to see which achieves the lowest RMPSE.

MODEL RESULTS

Findings

* Models with more than one AR terms achieve poorer forecasts, compared to the AR(1) models.
* Models incorporating student-t distributions achieve better
forecasts, with lower RMSPE.
* While ARCH tests (in the previous page) has shown that the ARCH effects are sufficiently accounted for in the ARCH(1) models, adding an additional lag (ARCH(2)) achieves a better forecast outcome.

We also modify the algorithm to run two simultaneous models and forecasts at the same time: the AR(1) and GARCH(1,2) under normal distribution and student t distribution. We then take the average sigma forecasts and calculated the RMSPE based on these. The output
shows that the model fails to perform with a lower RMPSE compared to the AR(1) GARCH(1,2) student t distribution.
Based on the output, we move forward with the AR(1) GARCH (1,2) under a student t distribution, as it achieves the lowest RMSPE amongst the base models.

The following outlines an output of the model.

```r
R Code for model
```

Baseline output
